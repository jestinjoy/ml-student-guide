# **Introduction**

### **Definition**

Machine Learning (ML) is a branch of Artificial Intelligence that enables computers to learn from data and improve their performance without being explicitly programmed.
It involves developing algorithms and models that can identify patterns in data and make predictions or decisions automatically.

In simple terms, Machine Learning allows a system to learn from experience, just as a human learns from practice. For example, an email application learns to detect spam messages by studying thousands of emails marked as spam or not spam.

In traditional programming, a programmer writes rules for the computer to follow. In Machine Learning, instead of writing rules, data is given to the computer. The computer then learns the rules or patterns by itself from that data.

Machine Learning systems improve their accuracy as they process more data. This makes them very useful for solving real-world problems where manual programming is difficult or impossible.

Artificial Intelligence, Machine Learning, Deep Learning, and Data Mining are closely related fields in computer science. **Artificial Intelligence** is a broad field of computer science that focuses on creating systems capable of performing tasks that normally require human intelligence. AI aims to make machines think and act intelligently.

**Machine Learning** is a subfield of Artificial Intelligence that allows computers to learn from data without being explicitly programmed. In Machine Learning, algorithms use past data to find patterns and make predictions on new data. It focuses mainly on learning from experience and improving accuracy over time.

**Deep Learning** is a subset of Machine Learning that uses artificial neural networks with many layers to learn complex patterns in large datasets. Deep Learning models are inspired by the structure of the human brain. They can automatically extract important features from raw data such as images, audio, or text.

**Data Mining** is the process of discovering hidden patterns, relationships, and useful information from large datasets. It is a superset of other methods and often uses statistical methods and Machine Learning algorithms to find trends or associations in data.

### **History**

The concept of machines that could learn was first discussed by **Alan Turing** in 1950 through his paper "Computing Machinery and Intelligence".The first computer program capable of learning, called the **Perceptron**, was developed by **Frank Rosenblatt** in 1957.

During 1960-80 researchers used logical rules and symbolic approaches to teach computers. However, these systems required exact instructions and could not handle large or noisy data.

During 1990's, with the growth of computer power, statistical models like Decision Trees and Support Vector Machines (SVM) became popular. These models could handle real-world data more effectively.

The availability of large datasets and the development of powerful computers led to a rise in the use of Neural Networks and Ensemble Methods in 2000's

The introduction of Deep Learning, which uses multiple layers of neural networks, revolutionized Machine Learning. It enabled major breakthroughs in image recognition, speech recognition, and natural language processing. Now companies like Google, Amazon, and Facebook began using Machine Learning widely in their products.
